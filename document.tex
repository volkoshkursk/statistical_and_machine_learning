\documentclass[11pt]{beamer}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[T2A]{fontenc}
\usepackage{cmbright}
\usepackage[russian]{babel}
\usetheme{Darmstadt}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{graphicx}
% Использовать полужирное начертание для векторов
\let\vec=\mathbf

\DeclareMathOperator{\mathspan}{span}
\DeclareMathOperator{\mathdim}{dim}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}
\begin{document}
	\author{Е. Ларин, Ф. Ежов, И. Кононыхин }
	\title{Обучение с учителем. Классификация. Дискриминантный анализ. }
	%\subtitle{}
	%\logo{}
	\institute{Санкт-Петербургский государственный университет 
		
		Прикладная математика и информатика
		
		Вычислительная стохастика и статистические модели
	}
	\date{}
	\subject{Семинар по статистическому и машинному обучению}
	\setbeamercovered{transparent}
	\setbeamertemplate{navigation symbols}{}
	\begin{frame}[plain]
		\maketitle 
	\end{frame}
	
	\begin{frame}
		\frametitle{Обучение с учителем}
		
		Выборка из генеральной случайной величины
		\begin{itemize}
			\item Для задачи регрессии: $\bm{X} \in \mathbb{R}^{n\times p}, \;\;\mathbf{y}\in \mathbb{R}^n$
		    \item Для задачи классификации: $\bm{X} \in \mathbb{R}^{n\times p}, \;\;\mathbf{y}\in \mathbb{A}^n$
        \end{itemize}
		
	\end{frame}
	\begin{frame}
		\frametitle{Обучение с учителем: формальная постановка}
		\begin{itemize}
			\item \textit{Вход}: $\bm{X}$ --- выборка $\bm{\xi}$, $\bm{y}$ --- выборка $\eta$. Предполагаем, что существует неизвестное отображение $y^*: \bm{\xi} \to \eta$  (гипотеза непрерывности или компактности)
			
			\item \textit{Задача}: По $\bm{X}$ и $\bm{y}$ найти такое отображение $\hat{y}^*: \bm{\xi} \to \eta$, которое приблизит отображение  $y^*$. 
			
			\item \textit{Оценка}: Функция потерь $\mathfrak{L}(y^*(x), \hat{y}^*(x))$. Здесь $x$ --- реализация $\bm{\xi}$
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\frametitle{Классификация}
		\begin{equation}
			\bm{X} \in \mathbb{R}^{n\times p}, \;\;\mathbf{y}\in \mathbb{A}^n
		\end{equation}
		\begin{block}{Гипотеза компактности}
			<<Близкие>> объекты, как правило, принадлежат одному классу
		\end{block}
		Понятие близости может быть формализовано, например, так:
		$$\rho(\bm{x_1}, \bm{x_2}) = \left(\displaystyle{\sum_{i = 1}^p w_i|x_1^i - x_2^i|^k}\right)^{{1}\over {k}}$$
	\end{frame}

	\begin{frame}
		\frametitle{Классификация: генеральная постановка}
		\textit{Дано:}
		\begin{itemize}
			\item $\bm{\xi} \in \mathbb{R}^p$ --- вектор признаков
			\item $\eta \in \mathbb{A}$ --- классовая принадлежность
		\end{itemize}
	
		Предположение об их зависимсти можно записать в виде \ref{2}.
		\begin{equation}
			\eta = \Phi(\bm{\xi}, \varepsilon)
			\label{2}
		\end{equation}
		Обычно на $\varepsilon$ накладываются условия $$E\varepsilon = 0, \;\; D\varepsilon = \sigma^2, \bm{\xi} \perp \varepsilon$$
		
		\textit{Задача:} найти $\Phi$
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: выборочная постановка}
		\textit{Дано:}
		\begin{itemize}
			\item $\bm{X} \in \mathbb{R}^{n\times p}$ --- матрица признаков
			\item $\bm{y} \in \mathbb{A}^n$ --- вектор классовой принадлежности
		\end{itemize}
		
		Предположение имеет вид \ref{3}.
		\begin{equation}
			y_i = \Phi(\bm{x}_i, \varepsilon_i),\;\;\; i = 1, \ldots, n
			\label{3}
		\end{equation}
		
		\textit{Задача:} найти $\Phi$
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: оценка качества}
		\begin{figure}
			\includegraphics[width=0.3\linewidth]{imgs/conf_matrix}
		\end{figure}
	    На основе этой матрицы есть большое количество разных метрик: \textit{accuracy}, \textit{recall}, \textit{precision}, $F_\beta$, \textit{ROC--AUC}
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: типы классов}
		\begin{itemize}
			\item По количеству классов:
			\begin{itemize}
				\item бинарная классификация
				\item многоклассовая классификация
			\end{itemize}
			\item По пересечению классов 
			\begin{itemize}
				\item пересекающиеся
				\item непересекающаяся
				\item нечёткие
			\end{itemize}
		\end{itemize}
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: этапы обучения модели}
		\begin{itemize}
			\item Выбор модели (класс рассматриваемых $\Phi$ из \ref{3})
			\item Выбор метрики
			\item Выбор метода обучения (способ подбора параметров для минимизации метрики на обучающем множестве)
			\item Выбор метода проверки (способ оценки качества модели)
		\end{itemize}
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: задача оптимизации}
		\begin{itemize}
			\item $\hat{\beta}$ --- параметры модели
			\item $\bm{\Phi}(\bm{x}, \beta)$ --- функционал классификации
			\item $\mathfrak{L}(\bm{\Phi}(\bm{x}, \beta), \bm{y})$ --- функция потерь (метрика)
		\end{itemize}
		\begin{block}{}
			$$\hat{\beta} = \arg\min_{\beta} \mathfrak{L}(\bm{\Phi}(\bm{x}, \beta), \bm{y})$$
		\end{block}
	
	\end{frame}
	\begin{frame}
		\frametitle{Классификация: общий подход к решению}
		Как построить функционал $\Phi$?
		
		
		Общий подход --- построить набор $f_i$, $i = 1, \ldots, K$. Каждая функция $f_i(\bm{x})$ показывает меру принадлежности $\bm{x}$ классу $i$. 
		
		Таким образом,
		\begin{equation}
			\Phi(\bm{x}) = \arg\max_i(f_i(\bm{x})).
			\label{4}
		\end{equation}
	\end{frame}
	\begin{frame}
		\frametitle{Дискриминантный анализ}
		Примем за функции $f_i$ из \ref{4} оценку вероятности принадлежности к $i$-му классу.
		
		$$\Phi(\bm{x}) = \arg\max_i (P(C_i|\bm{x})).$$
		
		$C_i$ ---  класс, состоящий из одного события: $\bm{x}$ принадлежит $i$-му классу.
	\end{frame}
	\begin{frame}
		\frametitle{Дискриминантный анализ}
		Если известны априорные вероятности получения $i$-го класса ($\pi_i$), применим формулу Байеса
		
		$$P(C_i|\bm{x}) = {{\pi_i P(\bm{x}|C_i)}\over{\sum_{j=1}^{K} \pi_j P(\bm{x}|C_j)}}.$$
		 
		 Отбросим знаменатель
		
		$$f_i = P(C_i|\bm{x}) = \pi_i P(\bm{x}|C_i).$$
	\end{frame}
	\begin{frame}
		\frametitle{LDA}
		\textit{Предположение:}	
		$$\mathtt{P}(\bm{\xi}|\eta = A_i) = \mathtt{N}(\bm{\mu}_i, \bm{\Sigma})$$
		
		\textit{Классифицирующая функция:}
		$$f_i(\bm{x}) = {{\pi_i}\over{(2\pi)^{p/2}|\bm{\Sigma}|^{1/2}}}exp\left(-{{1}\over{2}}(\bm{x} - \bm{\mu}_i)\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu}_i)^\mathtt{T}\right)$$
		
		\textit{После упрощения:}
		$$h_i(\bm{x}) = -0.5 \bm{\mu}_i\bm{\Sigma}^{-1}\bm{\mu}_i^\mathtt{T} + \bm{\mu}_i\bm{\Sigma}^{-1}\bm{x} + \log\pi_i$$

	\end{frame}

	\begin{frame}
		\frametitle{QDA}
		\textit{Предположение:}	
		$$\mathtt{P}(\bm{\xi}|\eta = A_i) = \mathtt{N}(\bm{\mu}_i, \bm{\Sigma}_i)$$
		
		\textit{Классифицирующая функция:}
		$$f_i(\bm{x}) = {{\pi_i}\over{(2\pi)^{p/2}|\bm{\Sigma}_i|^{1/2}}}exp\left(-{{1}\over{2}}(\bm{x} - \bm{\mu}_i)\bm{\Sigma}_i^{-1}(\bm{x} - \bm{\mu}_i)^\mathtt{T}\right)$$
		
		\textit{После упрощения:}
		$$g_i(\bm{x}) = -0.5 (\bm{x} - \bm{\mu}_i)\bm{\Sigma}^{-1}(\bm{x} - \bm{\mu}_i)^\mathtt{T} - 0.5\log|\bm{\Sigma}_i| + \log\pi_i$$
		
	\end{frame}



	\begin{frame}
		\frametitle{Место Феди}
		
	\end{frame}



	\begin{frame}
		\frametitle{Кросс-валидация}
		Кросс-валидация (aka перекрестная проверка, скользящий контроль) --- процедура эмпирического оценивания обобщающей способности алгоритмов. 
		
		С помощью кросс-валидации "эмулируется" наличие тестовой выборки, которая не участвует в обучении модели, но для которой известны правильные ответы.
		
	\end{frame}
	\begin{frame}
		\frametitle{Кросс-валидация: виды}
		\begin{itemize}
			\item Валидация на отложенных данных (Hold-Out Validation);
			\item Полная кросс-валидация (Complete Cross-Validation);
			\item k-fold Cross-Validation;
			\item t$\times$k-fold Cross Validation;
			\item Кросс-валидация по отдельным объектам (Leave-One-Out);
			\item Случайные разбиения (Random Subsampling).
		\end{itemize}
		
	\end{frame}

	\begin{frame}
		\frametitle{Кросс-валидация: Обозначения}
		Введем обозначения:
		\begin{itemize}
			\item $\bm{X}$ --- матрица признаков, описывающие таргеты $\bm{y}$;
			\item $\bm{T}^l = (x_i, y_i)_{i=1}^l, x_i \in \bm{X}, y_i \in \bm{y}$ --- обучающая выборка;
			\item $\mathfrak{L}$ --- функция потерь (мера качества);
			\item $A$ --- исследуемая модель;
			\item $\mu : (\bm{X}\times\bm{y})\rightarrow A$ --- алгоритм обучения.
		\end{itemize}
	\end{frame}
	\begin{frame}
		\frametitle{Кросс-валидация: Hold-Out Validation}
		Обучающая выборка один раз случайным образом разбивается на две части $\bm{T} = \bm{T}^t \cup \bm{T}^{l-t}$.
		\begin{figure}
			\includegraphics[width=1\linewidth]{imgs/hold-out}
		\end{figure}
		После чего решается задача оптимизации:
		$HO(\mu, \bm{T}^t, \bm{T}^{l-t}) = \mathfrak{L}(\mu(\bm{T}^t), \bm{T}^{l-t}) \rightarrow min$.
		
		
		Метод Hold-out применяется в случаях больших датасетов, т.к. требует меньше вычислительных мощностей по сравнению с другими методами кросс-валидации. 
		
		Недостатком метода является то, что оценка существенно зависит от разбиения, тогда как желательно, чтобы она характеризовала только алгоритм обучения.
	\end{frame}

	\begin{frame}
		\frametitle{Кросс-валидация: Complite cross-validation}
		\begin{itemize}
			\item Выбирается значение $t$;
			\item Выборка разбивается всевозможными способами на две части $\bm{T} = \bm{T}^t \cup \bm{T}^{l-t}$.
		\end{itemize}
		\begin{figure}
			\includegraphics[width=1\linewidth]{imgs/completecrossvalidation}
		\end{figure}
		После чего решается задача оптимизации:
		$CVV_t = \frac{1}{C_l^{l-t}}\sum_{\bm{T}^l = \bm{T}^t \cup \bm{T}^{l-t}} \mathfrak{L}(\mu(\bm{T}^t), \bm{T}^{l-t}) \rightarrow min$.
		
		
		Здесь число разбиений $ C_l^{l-t} $ становится слишком большим даже при сравнительно малых значениях $ t $, что затрудняет практическое применение данного метода.
	\end{frame}

    \begin{frame}
		\frametitle{Кросс-валидация: k-fold Cross-Validation}
		\begin{itemize}
			\item $\bm{T}^l$ разбивается на $ \bm{F_1}\cup\cdots\cup\bm{F_k}, |\bm{F_i}|\approx \frac{1}{k} $ частей;
			\item Производится $ k $ итераций:
			\begin{itemize}
				\item Модель обучается на $ k-1 $ части обучающей выборки;
				\item Модель тестируется на части обучающей выборки, которая не участвовала в обучении.
			\end{itemize}
		\end{itemize}
		Каждая из $ k $ частей единожды используется для тестирования.
		
		\begin{figure}
			\includegraphics[width=0.4\linewidth]{imgs/K-fold-validation}
		\end{figure}
		После чего решается задача оптимизации:
		$CV_k = \frac{1}{k}\sum_{i=1}^k\mathfrak{L}(\mu(\bm{T}^l \setminus \bm{F}_i), \bm{F}_i) \rightarrow min$.
	\end{frame}

	\begin{frame}
		\frametitle{Кросс-валидация: t$\times$k-fold Cross Validation}
		\begin{center}
			\textit{Как k-fold Cross-Validation, только $t$ раз.}
		\end{center}
		
		Разбиение:
		$\bm{T}^l = \bm{F}_{(1,1)}\cup\cdots\cup\bm{F}_{(k,1)}=\bm{F}_{(1,t)}\cup\cdots\cup\bm{F}_{(k,t)},|\bm{F}_{(i,j)}|\approx \frac{l}{k} $, 
		 
		Задача оптимизации: 
		$CV_{t\times k} = \frac{1}{tk}\sum_{j=1}^t\sum_{i=1}^k\mathfrak{L}(\mu(\bm{T}^l \setminus \bm{F}_{i,j}), \bm{F}_{i,j}) \rightarrow min$
	\end{frame}

	\begin{frame}
		\frametitle{Кросс-валидация: Leave-One-Out}
		Выборка разбивается на $ l-1 $ и $ 1 $ объект $ l $ раз.
		\begin{figure}
			\includegraphics[width=1\linewidth]{imgs/LeaveOneOut}
		\end{figure}
		$LOO = \frac{1}{l}\sum_{i=1}^l\mathfrak{L}(\mu(\bm{T}^l \setminus p_i), p_i) \rightarrow min$, где $ p_i = (x_i, y_i) $.
		
		Преимущества LOO в том, что каждый объект ровно один раз участвует в контроле, а длина обучающих подвыборок лишь на единицу меньше длины полной выборки.
		
		Недостатком LOO является большая ресурсоёмкость, так как обучаться приходится $ L $ раз.
	\end{frame}

	\begin{frame}
		\frametitle{Кросс-валидация: Random subsampling (Monte-Carlo cross-validation)}
		Выборка разбивается в случайной пропорции. Процедура повторяется несколько раз.
		\begin{figure}
			\includegraphics[width=1\linewidth]{imgs/completecrossvalidation}
		\end{figure}
	\end{frame}
	
	\begin{frame}
		\frametitle{Кросс-валидация: Выбор лучшей модели}
		Не переобученый алгоритм должен показывать одинаковую эффективность на каждой части.
	\end{frame}

		
\end{document}